{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/jljubas/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'月の地表の温度は何度'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def enforce_reproducibility(seed=42):\n",
    "    # Sets seed manually for both CPU and CUDA\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # For atomic operations there is currently\n",
    "    # no simple way to enforce determinism, as\n",
    "    # the order of parallel operations is not known.\n",
    "    # CUDNN\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # System based\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "enforce_reproducibility()\n",
    "\n",
    "dataset_train       = pd.read_parquet(\"dataset/train.parquet\")\n",
    "dataset_validation  = pd.read_parquet(\"dataset/validation.parquet\")\n",
    "\n",
    "questions_fi = dataset_train.loc[(dataset_train[\"lang\"] == \"fi\")][\"question\"].apply(lambda row: ['<s>'] + nltk.word_tokenize(row) + ['</s>']).to_list()\n",
    "questions_fi = [item for sublist in questions_fi for item in sublist]\n",
    "\n",
    "\n",
    "a = dataset_validation.loc[(dataset_validation[\"lang\"] == \"ja\")][\"question\"].to_list()\n",
    "a[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOT SURE.-[ 1-gram model ] - [ No smoothing ] - Finnish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-46.74348697097472"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_unigram_probabilities(train_dataset):\n",
    "    questions_fi = dataset_train.loc[(dataset_train[\"lang\"] == \"fi\")][\"question\"].apply(lambda row: nltk.word_tokenize(row, language='finnish')).to_list()\n",
    "    tokens = [item.lower() for sublist in questions_fi for item in sublist]\n",
    "    \n",
    "    counts = {}\n",
    "    for token in tokens:\n",
    "        if token not in counts.keys():\n",
    "            counts[token] = 1\n",
    "        else:\n",
    "            counts[token] += 1\n",
    "    token_list_size = len(tokens)\n",
    "\n",
    "    for count in counts:\n",
    "        counts[count] = counts[count] / token_list_size\n",
    "\n",
    "    return counts\n",
    "\n",
    "model = calc_unigram_probabilities(dataset_train)\n",
    "\n",
    "\n",
    "def unigram_model_fi(model,text):\n",
    "    log_sentence_probability = 0.0\n",
    "    OOV_default_prob = 1e-6\n",
    "    \n",
    "    for token in nltk.word_tokenize(text):\n",
    "        if token.lower() in model:\n",
    "            token_prob = model[token.lower()]  # Use a small value for unknown words\n",
    "            \n",
    "        else: \n",
    "            token_prob = OOV_default_prob\n",
    "        log_sentence_probability += np.log(token_prob)\n",
    "    \n",
    "    return log_sentence_probability\n",
    "\n",
    "\n",
    "unigram_model_fi(model,'Missä maassa Jack Churchill syntyi?')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finnish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FI-1.- [ 2-gram model ] - [ No smoothing ] - Finnish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-46.34585503574918"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_bigram_probabilities(train_dataset):\n",
    "    questions_fi = dataset_train.loc[(dataset_train[\"lang\"] == \"fi\")][\"question\"].apply(lambda row: ['<s>'] + nltk.word_tokenize(row, language='finnish') + ['</s>']).to_list()\n",
    "    tokens = [item.lower() for sublist in questions_fi for item in sublist]\n",
    "\n",
    "    counts = {}\n",
    "    for token in tokens:\n",
    "        if token not in counts.keys():\n",
    "            counts[token] = 1\n",
    "        else:\n",
    "            counts[token] += 1\n",
    "\n",
    "    bigram_probs = {}\n",
    "    for token_st, token_nd in zip(tokens,tokens[1:]):\n",
    "        bigram = (token_st,token_nd)\n",
    "        if bigram not in bigram_probs.keys():\n",
    "            bigram_probs[bigram] = 1\n",
    "        else:\n",
    "            bigram_probs[bigram] += 1\n",
    "\n",
    "    for bigram in bigram_probs:\n",
    "        first = bigram[0]\n",
    "        bigram_probs[bigram] = bigram_probs[bigram]/counts[first]\n",
    "\n",
    "    return bigram_probs\n",
    "\n",
    "model = calc_bigram_probabilities(dataset_train)\n",
    "\n",
    "def bigram_model_fi(model, text):\n",
    "    tokens = nltk.word_tokenize(text, language='finnish')\n",
    "    tokens = ['<s>'] + [token.lower() for token in tokens]  + ['</s>']\n",
    "\n",
    "    bigrams = [(token_st,token_nd) for token_st,token_nd in  zip(tokens,tokens[1:])]\n",
    "\n",
    "    log_sentence_probability = 0.0\n",
    "    OOV_default_prob = 1e-6\n",
    "    \n",
    "    for bigram in bigrams:\n",
    "        if bigram in model:\n",
    "            bigram_prob = model[bigram]  \n",
    "        else: \n",
    "            bigram_prob = OOV_default_prob\n",
    "        log_sentence_probability += np.log(bigram_prob)\n",
    "    \n",
    "    return log_sentence_probability\n",
    "\n",
    "bigram_model_fi(model,'Missä maassa Jack Churchill syntyi?')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FI-2.- [ 2-gram model ] - [ Laplace smoothing ] - Finnish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-57.188383868337404"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_bigram_probabilities(train_dataset):\n",
    "    questions_fi = dataset_train.loc[(dataset_train[\"lang\"] == \"fi\")][\"question\"].apply(lambda row: ['<s>'] + nltk.word_tokenize(row, language='finnish') + ['</s>']).to_list()\n",
    "    tokens = [item.lower() for sublist in questions_fi for item in sublist]\n",
    "\n",
    "    unigram_counts = {}\n",
    "    for token in tokens:\n",
    "        if token not in unigram_counts.keys():\n",
    "            unigram_counts[token] = 1\n",
    "        else:\n",
    "            unigram_counts[token] += 1\n",
    "\n",
    "    V = len(unigram_counts)\n",
    "\n",
    "   \n",
    "    bigram_counts = {}\n",
    "    for token_st, token_nd in zip(tokens, tokens[1:]):\n",
    "        bigram = (token_st, token_nd)\n",
    "        if bigram not in bigram_counts:\n",
    "            bigram_counts[bigram] = 1\n",
    "        else:\n",
    "            bigram_counts[bigram] += 1\n",
    "\n",
    "\n",
    "    bigram_probs = {}\n",
    "    \n",
    "  \n",
    "    for token_st in unigram_counts:\n",
    "        for token_nd in unigram_counts:\n",
    "            bigram = (token_st, token_nd)\n",
    "            count_bigram = bigram_counts[bigram] if bigram in bigram_counts else 0\n",
    "            bigram_probs[bigram] = (count_bigram + 1) / (unigram_counts[token_st] + V)\n",
    "    \n",
    "    return bigram_probs\n",
    "\n",
    "model = calc_bigram_probabilities(dataset_train)\n",
    "\n",
    "def bigram_model_ru(model, text):\n",
    "    tokens = nltk.word_tokenize(text, language='finnish')\n",
    "    tokens = ['<s>'] + [token.lower() for token in tokens]  + ['</s>']\n",
    "\n",
    "\n",
    "    bigrams = [(token_st,token_nd) for token_st,token_nd in  zip(tokens,tokens[1:])]\n",
    "\n",
    "    log_sentence_probability = 0.0\n",
    "    OOV_default_prob = 1e-6\n",
    "    \n",
    "    for bigram in bigrams:\n",
    "        if bigram in model:\n",
    "            bigram_prob = model[bigram]  \n",
    "        else: \n",
    "            bigram_prob = OOV_default_prob\n",
    "        log_sentence_probability += np.log(bigram_prob)\n",
    "    \n",
    "    return log_sentence_probability\n",
    "\n",
    "bigram_model_ru(model,'Missä maassa Jack Churchill syntyi?')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Russian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RU-1.- [ 2-gram model ] - [ No smoothing ] - Russian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-31.599046311471977"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_bigram_probabilities(train_dataset):\n",
    "    questions_fi = dataset_train.loc[(dataset_train[\"lang\"] == \"ru\")][\"question\"].apply(lambda row: ['<s>'] + nltk.word_tokenize(row, language='russian') + ['</s>']).to_list()\n",
    "    tokens = [item.lower() for sublist in questions_fi for item in sublist]\n",
    "\n",
    "    counts = {}\n",
    "    for token in tokens:\n",
    "        if token not in counts.keys():\n",
    "            counts[token] = 1\n",
    "        else:\n",
    "            counts[token] += 1\n",
    "\n",
    "    bigram_probs = {}\n",
    "    for token_st, token_nd in zip(tokens,tokens[1:]):\n",
    "        bigram = (token_st,token_nd)\n",
    "        if bigram not in bigram_probs.keys():\n",
    "            bigram_probs[bigram] = 1\n",
    "        else:\n",
    "            bigram_probs[bigram] += 1\n",
    "\n",
    "    for bigram in bigram_probs:\n",
    "        first = bigram[0]\n",
    "        bigram_probs[bigram] = bigram_probs[bigram]/counts[first]\n",
    "\n",
    "    return bigram_probs\n",
    "\n",
    "model = calc_bigram_probabilities(dataset_train)\n",
    "\n",
    "def bigram_model_ru(model, text):\n",
    "    tokens = nltk.word_tokenize(text, language='russian')\n",
    "    tokens = ['<s>'] + [token.lower() for token in tokens]  + ['</s>']\n",
    "\n",
    "    bigrams = [(token_st,token_nd) for token_st,token_nd in  zip(tokens,tokens[1:])]\n",
    "\n",
    "    log_sentence_probability = 0.0\n",
    "    OOV_default_prob = 1e-6\n",
    "    \n",
    "    for bigram in bigrams:\n",
    "        if bigram in model:\n",
    "            bigram_prob = model[bigram]  \n",
    "        else: \n",
    "            bigram_prob = OOV_default_prob\n",
    "        log_sentence_probability += np.log(bigram_prob)\n",
    "    \n",
    "    return log_sentence_probability\n",
    "\n",
    "bigram_model_ru(model,'Где вручается Шно́белевская премия ?')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RU-2.- [ 2-gram model ] - [ Laplace smoothing ] - Russian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-48.2739011526146"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_bigram_probabilities(train_dataset):\n",
    "    questions_fi = dataset_train.loc[(dataset_train[\"lang\"] == \"ru\")][\"question\"].apply(lambda row: ['<s>'] + nltk.word_tokenize(row, language='russian') + ['</s>']).to_list()\n",
    "    tokens = [item.lower() for sublist in questions_fi for item in sublist]\n",
    "\n",
    "    unigram_counts = {}\n",
    "    for token in tokens:\n",
    "        if token not in unigram_counts.keys():\n",
    "            unigram_counts[token] = 1\n",
    "        else:\n",
    "            unigram_counts[token] += 1\n",
    "\n",
    "    V = len(unigram_counts)\n",
    "\n",
    "   \n",
    "    bigram_counts = {}\n",
    "    for token_st, token_nd in zip(tokens, tokens[1:]):\n",
    "        bigram = (token_st, token_nd)\n",
    "        if bigram not in bigram_counts:\n",
    "            bigram_counts[bigram] = 1\n",
    "        else:\n",
    "            bigram_counts[bigram] += 1\n",
    "\n",
    "\n",
    "    bigram_probs = {}\n",
    "    \n",
    "  \n",
    "    for token_st in unigram_counts:\n",
    "        for token_nd in unigram_counts:\n",
    "            bigram = (token_st, token_nd)\n",
    "            count_bigram = bigram_counts[bigram] if bigram in bigram_counts else 0\n",
    "            bigram_probs[bigram] = (count_bigram + 1) / (unigram_counts[token_st] + V)\n",
    "    \n",
    "    return bigram_probs\n",
    "\n",
    "model = calc_bigram_probabilities(dataset_train)\n",
    "\n",
    "def bigram_model_ru(model, text):\n",
    "    tokens = nltk.word_tokenize(text, language='russian')\n",
    "    tokens = ['<s>'] + [token.lower() for token in tokens]  + ['</s>']\n",
    "\n",
    "\n",
    "    bigrams = [(token_st,token_nd) for token_st,token_nd in  zip(tokens,tokens[1:])]\n",
    "\n",
    "    log_sentence_probability = 0.0\n",
    "    OOV_default_prob = 1e-6\n",
    "    \n",
    "    for bigram in bigrams:\n",
    "        if bigram in model:\n",
    "            bigram_prob = model[bigram]  \n",
    "        else: \n",
    "            bigram_prob = OOV_default_prob\n",
    "        log_sentence_probability += np.log(bigram_prob)\n",
    "    \n",
    "    return log_sentence_probability\n",
    "\n",
    "bigram_model_ru(model,'Где вручается Шно́белевская премия ?')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Japanese"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JA-1.- [ 2-gram model ] - [ No smoothing ] - Japanese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-83.92396327421085"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def calc_bigram_probabilities(train_dataset):\n",
    "    tokenizer = Tokenizer()\n",
    "    questions_ja = train_dataset.loc[(dataset_train[\"lang\"] == \"ja\")][\"question\"].apply(lambda row: ['<s>'] + [token.surface for token in tokenizer.tokenize(row)] + ['</s>']).to_list()\n",
    "    tokens = [item.lower() for sublist in questions_ja for item in sublist]\n",
    "\n",
    "    counts = {}\n",
    "    for token in tokens:\n",
    "        if token not in counts.keys():\n",
    "            counts[token] = 1\n",
    "        else:\n",
    "            counts[token] += 1\n",
    "\n",
    "    bigram_probs = {}\n",
    "    for token_st, token_nd in zip(tokens,tokens[1:]):\n",
    "        bigram = (token_st,token_nd)\n",
    "        if bigram not in bigram_probs.keys():\n",
    "            bigram_probs[bigram] = 1\n",
    "        else:\n",
    "            bigram_probs[bigram] += 1\n",
    "\n",
    "    for bigram in bigram_probs:\n",
    "        first = bigram[0]\n",
    "        bigram_probs[bigram] = bigram_probs[bigram]/counts[first]\n",
    "\n",
    "    return bigram_probs\n",
    "\n",
    "model = calc_bigram_probabilities(dataset_train)\n",
    "\n",
    "def bigram_model_ja(model, text):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokens = [token.surface for token in tokenizer.tokenize(text)]\n",
    "    tokens = ['<s>'] + tokens  + ['</s>']\n",
    "\n",
    "    bigrams = [(token_st,token_nd) for token_st,token_nd in  zip(tokens,tokens[1:])]\n",
    "\n",
    "    log_sentence_probability = 0.0\n",
    "    OOV_default_prob = 1e-6\n",
    "    \n",
    "    for bigram in bigrams:\n",
    "        if bigram in model:\n",
    "            bigram_prob = model[bigram]\n",
    "        else: \n",
    "            bigram_prob = OOV_default_prob\n",
    "        log_sentence_probability += np.log(bigram_prob)\n",
    "    \n",
    "    return log_sentence_probability\n",
    "\n",
    "bigram_model_ja(model,'月の地表の温度は何度')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JA-2.- [ 2-gram model ] - [ Laplace smoothing ] - Japanese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-67.80043695279065"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_bigram_probabilities(train_dataset):\n",
    "    tokenizer = Tokenizer()\n",
    "    questions_ja = train_dataset.loc[(dataset_train[\"lang\"] == \"ja\")][\"question\"].apply(lambda row: ['<s>'] + [token.surface for token in tokenizer.tokenize(row)] + ['</s>']).to_list()\n",
    "    tokens = [item.lower() for sublist in questions_ja for item in sublist]\n",
    "\n",
    "    unigram_counts = {}\n",
    "    for token in tokens:\n",
    "        if token not in unigram_counts.keys():\n",
    "            unigram_counts[token] = 1\n",
    "        else:\n",
    "            unigram_counts[token] += 1\n",
    "\n",
    "    V = len(unigram_counts)\n",
    "\n",
    "   \n",
    "    bigram_counts = {}\n",
    "    for token_st, token_nd in zip(tokens, tokens[1:]):\n",
    "        bigram = (token_st, token_nd)\n",
    "        if bigram not in bigram_counts:\n",
    "            bigram_counts[bigram] = 1\n",
    "        else:\n",
    "            bigram_counts[bigram] += 1\n",
    "\n",
    "\n",
    "    bigram_probs = {}\n",
    "    \n",
    "  \n",
    "    for token_st in unigram_counts:\n",
    "        for token_nd in unigram_counts:\n",
    "            bigram = (token_st, token_nd)\n",
    "            count_bigram = bigram_counts[bigram] if bigram in bigram_counts else 0\n",
    "            bigram_probs[bigram] = (count_bigram + 1) / (unigram_counts[token_st] + V)\n",
    "    \n",
    "    return bigram_probs\n",
    "\n",
    "model = calc_bigram_probabilities(dataset_train)\n",
    "\n",
    "def bigram_model_ru(model, text):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = ['<s>'] + [token for token in tokens]  + ['</s>']\n",
    "\n",
    "\n",
    "    bigrams = [(token_st,token_nd) for token_st,token_nd in  zip(tokens,tokens[1:])]\n",
    "\n",
    "    log_sentence_probability = 0.0\n",
    "    OOV_default_prob = 1e-6\n",
    "    \n",
    "    for bigram in bigrams:\n",
    "        if bigram in model:\n",
    "            bigram_prob = model[bigram]  \n",
    "        else: \n",
    "            bigram_prob = OOV_default_prob\n",
    "        log_sentence_probability += np.log(bigram_prob)\n",
    "    \n",
    "    return log_sentence_probability\n",
    "\n",
    "bigram_model_ja(model,'月の地表の温度は何度')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
